# Order Processing Service

A Python-based microservice for processing orders and handling Kafka events using a clean architecture pattern with use cases, interfaces, and contracts.

## Features

- **Order Processing** â€“ Process incoming orders from Kafka events with validation, shipping calculation, discounts, and customer notifications
- **Order Cancellation** â€“ Cancel orders with refund calculation and customer notifications
- **Event-Driven Architecture** â€“ Kafka consumer with Dead Letter Queue (DLQ) support
- **Dynamic Event Processors** â€“ Automatically loads event processors based on event names
- **Clean Architecture** â€“ Separation of concerns with use cases, interfaces, and contracts
- **Centralized Logging** â€“ Abstract logger service with colored console output and file logging
- **Configurable Middleware** â€“ CORS, GZip, TrustedHost, and HTTPS redirect support

## Project Structure

```
src/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/                              # Business logic
â”‚   â”‚   â”œâ”€â”€ origin/                        # Shared base classes
â”‚   â”‚   â”‚   â””â”€â”€ entities/
â”‚   â”‚   â”‚       â”œâ”€â”€ abstract_usecase.py
â”‚   â”‚   â”‚       â”œâ”€â”€ abstract_factory.py
â”‚   â”‚   â”‚       â””â”€â”€ base_class.py
â”‚   â”‚   â”œâ”€â”€ orders/                        # Orders domain
â”‚   â”‚   â”‚   â”œâ”€â”€ entities/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ Order.py
â”‚   â”‚   â”‚   â””â”€â”€ features/
â”‚   â”‚   â”‚       â””â”€â”€ processOrder/
â”‚   â”‚   â”‚           â”œâ”€â”€ contracts/         # Concrete implementations
â”‚   â”‚   â”‚           â”œâ”€â”€ exceptions/        # Feature-specific exceptions
â”‚   â”‚   â”‚           â”œâ”€â”€ factory/           # Entry point / DI
â”‚   â”‚   â”‚           â”œâ”€â”€ interfaces/        # Abstract interfaces
â”‚   â”‚   â”‚           â”œâ”€â”€ schemas/           # Input/Output DTOs
â”‚   â”‚   â”‚           â”œâ”€â”€ services/          # Service orchestrators
â”‚   â”‚   â”‚           â””â”€â”€ usecases/          # Business use cases
â”‚   â”‚   â”‚       â””â”€â”€ cancelOrder/           # Same structure as processOrder
â”‚   â”‚   â”‚           â”œâ”€â”€ contracts/
â”‚   â”‚   â”‚           â”œâ”€â”€ exceptions/
â”‚   â”‚   â”‚           â”œâ”€â”€ factory/
â”‚   â”‚   â”‚           â”œâ”€â”€ interfaces/
â”‚   â”‚   â”‚           â”œâ”€â”€ schemas/
â”‚   â”‚   â”‚           â”œâ”€â”€ services/
â”‚   â”‚   â”‚           â””â”€â”€ usecases/
â”‚   â”‚   â””â”€â”€ system/                        # System-level utilities
â”‚   â””â”€â”€ infra/                             # Infrastructure layer
â”‚       â”œâ”€â”€ dotenv/                        # Environment configuration
â”‚       â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ logger/                        # Logging service
â”‚       â”‚   â”œâ”€â”€ interfaces/                # LoggerService interface
â”‚       â”‚   â”œâ”€â”€ contracts/                 # LoggerServiceContractV0
â”‚       â”‚   â””â”€â”€ services/                  # get_service_logger()
â”‚       â”œâ”€â”€ events/
â”‚       â”‚   â”œâ”€â”€ entities/
â”‚       â”‚   â”‚   â”œâ”€â”€ event.py
â”‚       â”‚   â”‚   â”œâ”€â”€ abstract_event_processor.py
â”‚       â”‚   â”‚   â””â”€â”€ concrete/              # Event processors (e.g., OrderCreated)
â”‚       â”‚   â””â”€â”€ features/
â”‚       â”‚       â””â”€â”€ process_events/        # Event processing feature
â”‚       â””â”€â”€ message_broker/
â”‚           â”œâ”€â”€ contracts/                 # Kafka consumer adapter
â”‚           â”œâ”€â”€ entities/                  # Abstract consumer adapter
â”‚           â”œâ”€â”€ functions/                 # Start/stop consumer functions
â”‚           â”œâ”€â”€ interfaces/                # Consumer interface
â”‚           â””â”€â”€ services/                  # Message broker service
â”œâ”€â”€ framework/                             # Framework entrypoints
â”‚   â”œâ”€â”€ entrypoints/
â”‚   â”‚   â”œâ”€â”€ server.py                      # FastAPI app with Kafka lifecycle
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ routes.py                  # API router
â”‚   â””â”€â”€ middlewares/                       # Custom middleware
â”‚       â”œâ”€â”€ debugger.py
â”‚       â””â”€â”€ route_error_handler.py
â””â”€â”€ tests/                                 # Test suite (mirrors src structure)
    â””â”€â”€ app/
        â””â”€â”€ core/
            â””â”€â”€ orders/
                â””â”€â”€ features/
                    â”œâ”€â”€ processOrder/
                    â”‚   â””â”€â”€ usecases/
                    â”‚       â””â”€â”€ test_USECASE_ProcessOrder.py
                    â””â”€â”€ cancelOrder/
                        â””â”€â”€ usecases/
                            â””â”€â”€ test_USECASE_CancelOrder.py
```

## Tech Stack

- **Python 3.12+**
- **FastAPI** â€“ Web framework
- **Pydantic** â€“ Data validation
- **Confluent Kafka** â€“ Kafka consumer/producer
- **pytest** â€“ Testing with async support
- **uv** â€“ Package manager

## Installation

```bash
# Install dependencies using uv
uv sync
```

## Configuration

Copy `.env.example` to `.env` and configure the following variables:

### Application Settings

| Variable         | Description           | Default                    |
| ---------------- | --------------------- | -------------------------- |
| `PRODUCTION_ENV` | Production mode flag  | `False`                    |
| `APP_NAME`       | Application name      | `Order Processing Service` |
| `APP_HOST`       | Server host           | `127.0.0.1`                |
| `APP_PORT`       | Server port           | `8000`                     |
| `APP_RELOAD`     | Hot reload (dev only) | `True`                     |
| `API_V1_STR`     | API version prefix    | `/api/v0`                  |
| `OPENAPI_URL`    | OpenAPI docs URL      | `/apidocs`                 |
| `LOG_FILE`       | Error log file path   | `error.log`                |

### Kafka Settings

| Variable                  | Description             | Default                  |
| ------------------------- | ----------------------- | ------------------------ |
| `KAFKA_BROKER_URL`        | Kafka broker address    | `localhost:9092`         |
| `KAFKA_GROUP_ID`          | Consumer group ID       | `order-processing-group` |
| `KAFKA_TOPIC`             | Topics to consume       | `["order-events"]`       |
| `KAFKA_DLQ_TOPIC`         | Dead Letter Queue topic | `orders-dlq`             |
| `KAFKA_AUTO_OFFSET_RESET` | Offset reset strategy   | `earliest`               |

### Middleware Settings

| Variable                       | Description             | Default |
| ------------------------------ | ----------------------- | ------- |
| `APP_MIDDLEWARE_CORS`          | Enable CORS middleware  | `True`  |
| `APP_MIDDLEWARE_GZIP`          | Enable GZip compression | `False` |
| `APP_MIDDLEWARE_TRUSTEDHOST`   | Enable trusted hosts    | `True`  |
| `APP_MIDDLEWARE_HTTPSREDIRECT` | Force HTTPS redirect    | `False` |

### CORS Settings

| Variable                   | Description           | Default |
| -------------------------- | --------------------- | ------- |
| `BACKEND_CORS_ORIGINS`     | Allowed origins       | `["*"]` |
| `BACKEND_CORS_CREDENTIALS` | Allow credentials     | `True`  |
| `BACKEND_CORS_METHODS`     | Allowed HTTP methods  | `["*"]` |
| `BACKEND_CORS_HEADERS`     | Allowed headers       | `["*"]` |
| `BACKEND_ALLOWED_HOSTS`    | Trusted host patterns | `["*"]` |

## Running the Service

### Development

```bash
uv run python3 main.py
```

### Production (Uvicorn)

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
```

### Production (Gunicorn + Uvicorn)

```bash
gunicorn main:app --workers 4 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

## Running Tests

```bash
# Run all tests
uv run pytest tests/ -v

# Run specific test file
uv run pytest tests/app/core/orders/features/processOrder/usecases/test_USECASE_ProcessOrder.py -v
```

## Architecture

### Architecture Philosophy

This codebase implements **Vertical Slice Architecture** combined with **Clean Architecture** principles, designed for **feature-driven development** in enterprise environments.

#### Why This Structure?

| Common First Impression          | Actual Intent                                                         |
| -------------------------------- | --------------------------------------------------------------------- |
| "Over-engineered"                | Deliberate vertical slice architecture for scalable team development  |
| "Too many files"                 | Proper separation of concerns â€“ each file has a single responsibility |
| "Java patterns forced on Python" | Framework-agnostic patterns that work in any language                 |

#### Design Decisions

1. **Vertical Slices over Horizontal Layers**

   - Each feature (`processOrder`, `cancelOrder`) is a self-contained unit
   - Adding a new feature = adding a new folder, not modifying existing code
   - Deleting a feature = deleting a folder, with zero impact on other features

2. **Explicit Naming Convention**

   - `USECASE_`, `SERVICE_`, `CONTRACT_`, `INTERFACE_` prefixes make the role of each class immediately clear
   - Self-documenting code that scales across large teams
   - Enables quick navigation in large codebases

3. **Dependency Inversion**

   - Business logic (use cases) depends on abstractions (interfaces), not implementations
   - Contracts can be swapped without changing business logic (e.g., `V0` â†’ `V1`)
   - Enables easy testing with mock implementations

4. **Factory Pattern for DI**
   - No framework dependency for dependency injection
   - Explicit wiring makes the dependency graph visible
   - Singleton pattern for performance-critical services

This architecture is particularly suited for:

- ğŸ¢ **Enterprise microservices** with multiple teams
- ğŸ”„ **Polyglot environments** (same patterns in Python, Java, TypeScript)
- ğŸ“ˆ **Rapidly evolving products** where features are added/removed frequently

---

### Clean Architecture Layers

| Layer          | Description                                                        |
| -------------- | ------------------------------------------------------------------ |
| **Entities**   | Domain models (e.g., `Order`, `Event`)                             |
| **Use Cases**  | Business logic (e.g., `USECASE_ProcessOrder`)                      |
| **Interfaces** | Abstract contracts (e.g., `INTERFACE_HELPER_ProcessOrder`)         |
| **Contracts**  | Concrete implementations (e.g., `CONTRACT_HELPER_ProcessOrder_V0`) |
| **Services**   | Orchestrators that wire use cases with helpers                     |
| **Factory**    | Entry points with DI and singleton pattern                         |

### Vertical Slice Architecture

Each feature is a **self-contained vertical slice** containing all layers:

```
features/
â””â”€â”€ processOrder/           â† Vertical Slice (self-contained feature)
    â”œâ”€â”€ schemas/            â† Input/Output DTOs
    â”œâ”€â”€ interfaces/         â† Ports (abstractions)
    â”œâ”€â”€ contracts/          â† Adapters (implementations)
    â”œâ”€â”€ exceptions/         â† Feature-specific errors
    â”œâ”€â”€ usecases/           â† Business logic
    â”œâ”€â”€ services/           â† Orchestration
    â””â”€â”€ factory/            â† Entry point / DI
```

**Benefits:**

- âœ… **Self-contained** â€“ All layers within one folder
- âœ… **Independently deployable** â€“ Clear boundaries
- âœ… **Easy to delete** â€“ Remove folder = remove feature
- âœ… **Parallel development** â€“ Teams can work on different features

### Order Processing Workflow

The `USECASE_ProcessOrder` executes the following steps:

1. **Load Order** â€“ Retrieve order by ID from the data source
2. **Validate Order** â€“ Ensure order is in a processable state (`new` or `pending`)
3. **Calculate Shipping** â€“ Determine shipping cost (free for orders â‰¥ $100)
4. **Apply Discounts** â€“ Apply percentage-based discounts (default 10%)
5. **Calculate Final Total** â€“ `total_amount + shipping - discount`
6. **Update Order Status** â€“ Set status to `processing`
7. **Notify Customer** â€“ Send notification with order details

### Order Cancellation Workflow

The `USECASE_CancelOrder` executes the following steps:

1. **Load Order** â€“ Retrieve order by ID from the data source
2. **Validate Cancellable** â€“ Ensure order can be cancelled (`new`, `pending`, or `processing` status)
3. **Calculate Refund** â€“ Determine refund amount (order total - 5% cancellation fee)
4. **Update Order Status** â€“ Set status to `cancelled`
5. **Notify Customer** â€“ Send cancellation notification with refund details

## Logging Service

The service uses a centralized logging infrastructure with:

- **Abstract Interface** â€“ `LoggerService` with `debug`, `info`, `warning`, `error`, `critical` methods
- **Colored Console Output** â€“ Different colors for each log level for better visibility
- **File Logging** â€“ Errors are logged to the configured `LOG_FILE`
- **Singleton Pattern** â€“ `get_service_logger()` returns a cached logger instance

### Usage

```python
from src.app.infra.logger.services.service_logger import get_service_logger

logger = get_service_logger()
logger.info("Processing order...")
logger.error("Failed to process order")
```

## Event Processing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Topic    â”‚â”€â”€â”€â”€â–¶â”‚  KafkaConsumerAdapter  â”‚â”€â”€â”€â”€â–¶â”‚  SERVICE_ProcessEvents       â”‚
â”‚  (order-events) â”‚     â”‚  (poll_messages)       â”‚     â”‚  (USECASE_ProcessEvents)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚                               â”‚
                                   â”‚ on error                      â”‚ detect_event_processor
                                   â–¼                               â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  DLQ Topic      â”‚           â”‚  EventProcessor_<EventName>  â”‚
                        â”‚  (orders-dlq)   â”‚           â”‚  (e.g., EventProcessor_      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   OrderCreated)              â”‚
                                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **Kafka Consumer** polls messages from configured topics
2. **SERVICE_ProcessEvents** processes each event through the use case
3. **detect_event_processor** dynamically loads the appropriate processor based on event name
4. Failed messages are sent to the **Dead Letter Queue (DLQ)**

## Adding New Event Processors

1. Create a new file in `src/app/infra/events/entities/concrete/` named after the event (e.g., `OrderShipped.py`)
2. Implement the `EventProcessor_<EventName>` class:

```python
from src.app.infra.events.entities.abstract_event_processor import AbstractEventProcessor
from src.app.infra.events.entities.event import Event
from src.app.infra.logger.services.service_logger import get_service_logger

class EventProcessor_OrderShipped(AbstractEventProcessor):
    def __init__(self):
        self._logger = get_service_logger()

    async def process(self, event: Event) -> None:
        order_id = event.data.get("orderId")
        self._logger.info(f"Processing OrderShipped event for order ID: {order_id}")
        # Your processing logic here
```

## Kafka Event Schema

Events consumed from Kafka follow this structure:

```json
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "name": "OrderCreated",
  "data": {
    "orderId": "12345",
    "customerId": "cust-789",
    "totalAmount": 150.0,
    "currency": "INR",
    "status": "CREATED",
    "items": [
      {
        "productId": "prod-001",
        "quantity": 2,
        "unitPrice": 75.0,
        "totalPrice": 150.0
      }
    ],
    "createdAt": "2026-01-05T11:34:53.145Z"
  }
}
```

| Field              | Type   | Description                                       |
| ------------------ | ------ | ------------------------------------------------- |
| `id`               | UUID   | Unique event identifier                           |
| `name`             | string | Event type (e.g., `OrderCreated`, `OrderShipped`) |
| `data`             | object | Event-specific payload                            |
| `data.orderId`     | string | Order identifier                                  |
| `data.customerId`  | string | Customer identifier                               |
| `data.totalAmount` | float  | Order total                                       |
| `data.currency`    | string | Currency code (ISO 4217)                          |

## Failure Handling & Reliability

### Implemented Failsafe Mechanisms

| Mechanism                   | Implementation                                                  |
| --------------------------- | --------------------------------------------------------------- |
| **Connection Retry**        | 5 attempts with exponential backoff (1s â†’ 2s â†’ 4s â†’ 8s â†’ 16s)   |
| **Correlation IDs**         | UUID generated per message for distributed tracing `[abc12345]` |
| **Safe JSON Parsing**       | Invalid JSON caught and routed to DLQ                           |
| **Double-Close Prevention** | `_consumer_closed` flag prevents errors                         |
| **DLQ with Metadata**       | Headers include `correlation_id` and `error_reason`             |
| **Graceful Shutdown**       | Interruptible sleep, stop flag checked on each iteration        |
| **Poll Auto-Restart**       | While loop with 5s delay replaces recursive calls               |

### Error Handling Flow

```
Message Received
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Invalid JSON    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JSON Parsing   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚     DLQ     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  (headers)  â”‚
         â”‚                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Valid JSON                         â–²
         â–¼                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Processing      â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Event Processor â”‚    Failed
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Success
         â–¼
   Message Logged
```

### Dead Letter Queue (DLQ)

Failed messages are routed to `orders-dlq` topic with:

- `correlation_id` header for tracing
- `error_reason` header (truncated to 500 chars)
- Original message key and value preserved

### Future Enhancements

| Enhancement           | Description                                     |
| --------------------- | ----------------------------------------------- |
| Health Check Endpoint | Kubernetes readiness probe integration          |
| Manual Offset Commit  | At-least-once delivery guarantee                |
| Circuit Breaker       | Prevent cascade failures to downstream services |
| Consumer Scaling      | Horizontal scaling via topic partitioning       |

## Interview Talking Points

<details>
<summary>Click to expand prepared answers for common interview questions</summary>

### "Describe the architecture"

> "This is a Python FastAPI service that consumes Kafka events and executes domain use cases. Domain logic is isolated from messaging and framework concerns using Clean Architecture with Vertical Slices. Infrastructure concerns are implemented via interfaces and concrete adapters."

### "Why Clean Architecture?"

> "It enables independent testing of business rules, isolates side-effects, and makes it easy to swap implementations (e.g., Kafka â†’ RabbitMQ, Mock â†’ PostgreSQL) without changing core logic. Each feature is a self-contained vertical slice that can be developed, tested, and deployed independently."

### "Explain DLQ & reliability"

> "Messages that fail processing get routed to a Dead Letter Queue. This ensures problematic messages are observable and debuggable without halting the pipeline. In production, I'd add retry logic with exponential backoff before DLQ routing."

### "How would you scale this?"

> "The consumer group design supports parallel processing. I'd partition topics based on order ID for ordering guarantees, scale consumers horizontally, and use Kubernetes HPA based on consumer lag metrics."

### "What happens if Kafka is down?"

> "On startup, the consumer would retry connection with backoff. I'd implement a health check that reports 'unhealthy' until Kafka is available. In production, I'd add circuit breaker patterns to prevent cascade failures."

</details>

## Naming Conventions

| Type            | Pattern                                        | Example                           |
| --------------- | ---------------------------------------------- | --------------------------------- |
| Use Case        | `USECASE_<FeatureName>`                        | `USECASE_ProcessOrder`            |
| Interface       | `INTERFACE_HELPER_<FeatureName>`               | `INTERFACE_HELPER_ProcessOrder`   |
| Contract        | `CONTRACT_HELPER_<FeatureName>_V<N>`           | `CONTRACT_HELPER_ProcessOrder_V0` |
| Schema          | `INPUT_<FeatureName>` / `OUTPUT_<FeatureName>` | `INPUT_ProcessOrder`              |
| Service         | `SERVICE_<FeatureName>`                        | `SERVICE_ProcessOrder`            |
| Event Processor | `EventProcessor_<EventName>`                   | `EventProcessor_OrderCreated`     |

> **Note:** These prefixes are intentional, not a deviation from PEP 8. They provide self-documenting code where the role of each class is immediately clear at a glance. This pattern is particularly valuable in polyglot environments where the same conventions work across Python, Java, and TypeScript codebases.

## API Documentation

When running in development mode (`PRODUCTION_ENV=False`), Swagger UI is available at:

- **Swagger UI**: `http://localhost:8000/docs`

## License

MIT
